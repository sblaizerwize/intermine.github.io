'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/apis/','title':"API documentation",'content':"BlueSnap Implementation Last updated: 23/01/2020\n Purpose This document explains the implementation of BlueSnap payment processor with SampleClient web services. It describes the payment logic, provides use cases, and shows the payment sequence diagrams for each case.\nAudience The target audience of this document includes stakeholders, developers, and project managers from SampleClient and MyEnterprise.\n Overview SampleClient requires to integrate BlueSnap as an alternative payment processor for its web services. SampleClient payment processors include:\n Stripe Humboldt Paysafe Safecharge  The first stage of BlueSnap implementation with SampleClient enables payments using a credit card. This document describes the payment flow of a New Vaulted Shopper Making a Purchase.\n A New Vaulted Shopper Making a Purchase The scenario for this purchase case is the following:\n A new shopper signs up for the first-time to the SampleClient web services. The shopper decides to buy credits to have access to SampleClient content using a credit card. The shopper selects the number of credits and proceeds to the checkout form. The SampleClient payment logic selects BlueSnap as payment processor from among several processors (Stripe, Humboldt, Paysafe, and Safecharge) to make a purchase with a credit card. The shopper fills up all required fields to complete the transaction. The payment processor asks permission to store the credit card information for future transactions. The shopper clicks the Buy securely now button. The shopper receives a notification status about the purchase transaction.   Sequence Diagram The following diagram shows the payment logic for a shopper making a purchase using BlueSnap payment processor.\nFigure 1. Payment Sequence Diagram for a Shopper Making a Purchase using BlueSnap Processor\nThe process of the payment sequence for a shopper making a purchase is the following:\n The frontend sends a GET request to the backend to load the BlueSnap payment form with the Hosted Payment Fields. The backend sends a POST request to the BlueSnap API to retrieve a Hosted Payment Fields token (pfToken). The BlueSnap API returns the pfToken on the Location header. The backend returns to the frontend the following parameters: pfToken, base_url, and billing_workflow. The frontend loads the checkout form with BlueSnap Hosted Payment Fields. The shopper clicks the Buy securely now button after filling out all of the fields. The frontend sends three PUT requests to the BlueSnap API to protect customer-sensitive data (CCN, Expiration date, and CVV). The frontend receives a confirmation of the binding between the pfToken and shopper’s card information. The frontend sends a POST request to backend to make a purchase using the pfToken and shopper’s payment details. The backend sends a POST request to BlueSnap API to vault a new shopper. The backend receives a BlueSnap Customer ID associated with the new vaulted shopper. The backend sends a POST request to BlueSnap API to make a purchase using the BlueSnap Customer ID. The backend receives a notification of the payment status. The backend sends back the notification of the payment status to the frontend. The frontend displays a window with the payment status.   Endpoints The following table lists the endpoints involved in the payment logic described above:\n  From   To   Endpoint     Frontend    Backend    GET /v2/billing/creditcard      Backend    BlueSnap API    POST /payment-fields-tokens      Frontend    BlueSnap API    PUT /payment-fields-tokens/      Frontend    Backend    POST /v2/billing/creditcard      Backend    BlueSnap API    POST /vaulted-shoppers      Backend   BlueSnap API    POST /transactions    GET /v2/billing/creditcard This request enables the frontend to receive a transaction token and load the BlueSnap payment form.\nURL http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1 \\ Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Authorization  Specifies the bearer token for SampleClient API authentication. Value: Bearer [USER_TOKEN].  string  required    Content-Type  Specifies the format of the content response. Value: application/x-www-form-urlencoded.  string  required    cache-control  Indicates whether there is a cache control. Value: cache and no-cache.  string  required    Query Parameters The following table lists the query parameters used in the request.\n  Parameter  Description  Type  Required / Optional    is_mobile  Specifies whether the shopper is using a mobile device. If true, value: 1.  integer  required    packages  Specifies the number of credits the shopper is buying. Value: 5, 11, 22.  integer  required    Sample Request The following is an example of a command-line request.\ncurl -X GET \\ \u0026#39;http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1\u0026amp;packages=11\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ -H \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ Response Body The following table lists the elements commonly used from the response.\n  Element  Description  Type    bluesnap_pars  Contains information on the transaction token.  data object      base_url  Specifies the base API URLs for the BlueSnap environments. Values: sandbox and production.  string      hostedFieldToken  Specifies the generated transaction token.  long    rebuy  Contains information about the stored payment methods of a vaulted shopper.  array    Important:\nThe elements listed in the response body table are in blue color in the Sample Response Body.  Sample Response Body The following is an extract of a typical response showing some of the elements returned for this request.\n{ \u0026#34;creditcardform\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;multi\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;packages\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Name on Credit Card\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_fullname\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Street Address\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;street\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;City\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Beverly Hills\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Location \u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;225\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: -1, \u0026#34;name\u0026#34;: \u0026#34;--------------\u0026#34; }, { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;AFGHANISTAN\u0026#34; }, ... { \u0026#34;id\u0026#34;: 239, \u0026#34;name\u0026#34;: \u0026#34;ZIMBABWE\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;country\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;ZIP/Postal Code\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;90210\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;postal_code\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;rebuy\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rebuy\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MIC\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;fic_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MPlus\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;mplus_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Check here to top up your credits when you run out so that you are ready for any encounter.\u0026#34;, \u0026#34;value\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;autobill_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card number\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_number\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;01\u0026#34; }, ... { \u0026#34;id\u0026#34;: 12, \u0026#34;name\u0026#34;: \u0026#34;12\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_month\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 2019, \u0026#34;name\u0026#34;: \u0026#34;2019\u0026#34; }, ... { \u0026#34;id\u0026#34;: 2068, \u0026#34;name\u0026#34;: \u0026#34;2068\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_year\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Card Verification Code (CVV) #\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_cvv\u0026#34; } ] } POST /payment-fields-tokens This method enables to create a Hosted Payment Fields token (pfToken) when using Hosted Payment Fields. It is a server-to-server request from SampleClient to BlueSnap when the customer access the checkout form. The token serves to protect sensitive shopper information during a transaction. The transactions using a pfToken are:\n Processing a purchase. Creating a vaulted shopper. Updating a vaulted shopper with a new credit card.  Important:\nThe Hosted Payment Fields token expires after 60 minutes.  URL https://sandbox.bluesnap.com/services/2/payment-fields-tokens Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Content-Type  Specifies that the format of the response body is a JSON object.  Value: application/json.   string  required    Accept  Specifies that the format of the accepted request is a JSON object.  Value: application/json.  string  required    Authorization  Specifies the bearer token for BlueSnap API authentication. Value: Bearer [USER_TOKEN]  string  required    Sample Request The following is an example of a command-line request.\ncurl -I -X POST \\ https://sandbox.bluesnap.com/services/2/payment-fields-tokens \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Accept: application/json\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ Sample Response Body The following is an example of a typical response that shows all the elements of the Hosted Payment Field token generation.\nHTTP/1.1 201 201 Date: Wed, 27 Nov 2019 17:24:47 GMT Set-Cookie: JSESSIONID=XXXXXXXX; Path=/services; Secure; HttpOnly Location: https://sandbox.bluesnap.com/services/2/payment-fields-tokens/XXXXXXXX_ Content-Length: 0 Strict-Transport-Security: max-age=31536000 ; includeSubDomains Set-Cookie: XXXXXXX; Path=/ Set-Cookie: XXXXXXX; path=/services Via: 1.1 sjc1-bit32 Hosted Payment Fields Token Errors The following table lists the errors returned from a request related to the Hosted Payment Field token.\n  Code  Name  Description    14040  EXPIRED_TOKEN  Token is expired    14041  TOKEN_NOT_FOUND  Token is not found    14042  NO_PAYMENT_DETAILS_LINKED_TO_TOKEN  Token is no associated with a payment method    "});index.add({'id':1,'href':'/docs/architecture/','title':"Architecture Guide",'content':"Architecture Guide Betelgeuse Last updated: 23/11/2020\n Purpose The purpose of this guide is to describe the software architecture of the Betelgeuse application and its interaction with third-party applications.\nAudience This document is useful to the developers and stakeholders of Orion who are implementing or updating new features for the Betelgeuse application.\n Introduction Betelgeuse is a serverless application hosted in the AWS cloud. Unlike the monolithic architecture of the previous Betelgeuse system, Betelgeuse is implemented as a containerized .NET application following a microservice-based architectural style. This style decomposes the monolithic approach into three main components, each associated with a microservice: bellatrix, rigel, and saiph. Each microservice undertakes a single business responsibility from the old Betelgeuse system as follows:\n Bellatrix Microservice: Handles transactions of individuals such as create/update clients and group leaders. It also manages brands, regions, routes, and groups. Rigel Microservice: Handles transactions related to credits and payments. Saiph Microservice: Handles the communication of Betelgeuse with third-party systems and processes transaction reports.  Although microservices run independently and, therefore, are built separately, they continuously interact among themselves. The microservices are built as container images orchestrated by the AWS Elastic Container Service (ECS). AWS Fargate provisions the infrastructure of the ECS clusters. Each microservice has its own database decoupled from the other microservices. Data is migrated from the previous Betelgeuse system and stored in serverless Aurora MySQL relational databases. Moreover, communication among the microservices is asynchronous and based on integration events. It follows the principles of request-response messaging for publish-subscribe channels, using both Amazon SNS topics and Amazon SQS queues.\nBetelgeuse is deployed on the cloud using the Octopus Deploy tools. GitLab CI/CD tools ensure continuous integration and continuous delivery of the application by implementing automated pipelines. In addition, Betelgeuse uses Terraform templates to implement Infrastructure as Code on AWS.\nFinally, the Betelgeuse client application is stored in a bucket of Amazon S3 and served as a Single Page Application (SPA) using AWS Cloudfront. The components of the client application are built using the Angular Framework.\nThe Betelgeuse application comprises the following modules:\n Betelgeuse Identity and Authorization Module Betelgeuse Web Frontend Betelgeuse Backend Betelgeuse Security Betelgeuse Monitoring  Figure 1 provides a high-level description of the Betelgeuse application.\nFigure 1. Betelgeuse Architecture\nThe architectural style of Betelgeuse follows the good practices of Clean Architecture. This style is suitable for complex business logics that involve several areas. The advantages of the Betelgeuse architecture are the following:\n It follows the Dependency Inversion Principle (DIP). This principle states that high-level modules should not depend on low-level modules. The interplay between both modules must be thorugh abstractions. Therefore, high-level modules of the Betelgeuse such as data, infrastructure, dependencies, and user interface are decoupled from low-level modules (business rules). This principle focuses the Betelgeuse architecture on the business logic of Orion. It complies with a Domain Driven Design (DDD). DDD is a software development approach that links the implementation of the application to the core business concepts of the client. This approach enables to map the old monolithic architecture of Betelgeuse consisting of use cases into domains. Each domain then correlates to a microservice in the new model of Betelgeuse. It is scalable. Microservices can scale up as Orion business logic does. Each microservice can assume a single business responsibility.   Betelgeuse Identity and Authorization Module This section describes the business logic of Betelgeuse for an end user to access and navigate the application according to their role in Orion.\nAmazon Cognito is the AWS service that enables authentication and authorization of Betelgeuse end users. Cognito user pools verify the identity of users so that they can directly sign in and sign up to the application. A user pool enables Betelgeuse to create and maintain an end-user directory that includes information such as profile, company, zone, and office.\nBetelgeuse, includes two methods to authenticate an end user:\n Directly registering the new user’s profile in the AWS Cognito console. Using a third-party SAML 2.0 identity provider, such as Azure Active Directory, for integrating Office 365 single sign-on (SSO), as Figure 2 shows.  Figure 2. Authentication Flow of Betelgeuse Users using Office 365\nThe user authentication flow using a third-party identity provider is the following:\n The user enters the domain of the Betelgeuse application. The application redirects the user to the Amazon Cognito hosted UI to authenticate using Office 365 SSO. Amazon Cognito redirects the user to the Microsoft login site to enter their Office 365 credentials. If the user authenticates successfully, Azure Active Directory provides four claims related to the user profile: role, office, zone, and company to Amazon Cognito using the SAML 2.0 standard, an industry standard for federated authentication.  Note:\nOriginally, Orion’s user information was kept on a local Windows server. Eventually, the user directory was uploaded to the Azure Active Directory in the cloud using the Azure AD connect service.  Next, Cognito issues a JSON Web Token (JWT) to the Betelgeuse application using the OAuth 2.0 implicit grant flow, which enables Cognito to directly return a JWT to Betelgeuse after the user successfully authenticates. The OAuth 2.0 standard is used to control user authorization. The Amplify framework decodes and verifies the JWT. If valid, it grants the user access to the application.  Important:\nThe Betelgeuse application uses the SDKs and libraries provided by the AWS Amplify framework to integrate AWS services. Amplify also handles the Betelgeuse sign in and sign up flow by implementing direct calls to the methods of the Auth class using the SDK of Angular CLI.   Betelgeuse Frontend This section describes the elements, along with their properties and relationships, that support the client and static content of Betelgeuse. An end user interacts with these elements after successfully identifying and accessing the application.\nThe Betelgeuse application uses the Angular JavaScript framework to build a Single-Page web Application (SPA). Amazon CloudFront serves the client and static content of the application, stored in an S3 Bucket. Betelgeuse Frontend communicates to the backend microservices rigel, bellatrix, and saiph using a REST API Interface.\nA CloudFront distribution is a reliable and secure way to serve content more efficiently because it enables Betelgeuse to cache the most frequently requested content. Each CloudFront distribution uses anSSL certificate to identify the Betelgeuse site and secure its private network communications. Moreover, each deployment environment: development, laboratory, testing, and production has its own CloudFront distribution.\nFrontend DNS (Domain Name System) requests are routed to the CloudFront distribution using the Amazon Route 53 service by means of an alternate DNS domain. The CloudFront distribution points to the origin S3 Bucket that hosts the client application. The S3 Bucket is configured for website hosting with policies to grant external access. For more details, check the section Routing Traffic to CloudFront Distributions.\n Betelgeuse Backend This section describes the following topics:\n The architecture of the microservices as containerized images along with their infrastructure provisioning The routing of incoming traffic from the Frontend The structure of the Virtual Private Cloud (VPC) and subnets The request-response messaging among microservices The deployment of the Betelgeuse application   Containerized-Based Microservices Betelgeuse uses a microservice-based architecture. Each microservice is a containerized application that has its own database. Betelgeuse includes three main microservices: bellatrix, rigel, and saiph. These microservices exist in all the deployment environments of the application: development, laboratory, testing, and production.\nFigure 3. Interplay between the ECS Cluster and AWS Fargate for Building the Microservices.\nAs Figure 3 shows, each deployment environment is built on an AWS ECS (Elastic Container Service) cluster and launched using AWS Fargate. Amazon ECS is an orchestration service that handles your containers using services. Services define tasks based on a task definition, which works as a blueprint that describes how to provision your containers. Tasks are one-time executions of your containers. For example, the development environment includes an ECS cluster with three ECS services:\n orion-dev-bellatrix orion-dev-rigel orion-dev-saiph  Each service can contain a single or multiple tasks representing a container image of a specific microservice. You can scale up to any number of tasks for maintaining your application’s availability if a task failure occurs.\nAs mentioned above, the ECS service builds a container image of a microservice using a task definition. A task definition is required to run Docker images in ECS. The task definition defines some parameters such as the origin Docker image, CPU, and memory for each task, launch type, and ports. Betelgeuse uses AWS ECR (Elastic Container Registry) to store the Docker images implemented and built in GitLab by the developers. The ECS service consumes the Docker images from the AWS ECR and builds the container image of the microservices using the task definition. For more details, consult the Deployment section. To ensure compatibility and usage of the latest image version, GitLab uses semantic versioning (semver) of images.\nAWS Fargate enables you to run your containers without requiring the whole capacity of EC2 instances. Fargate handles the infrastructure required for each ECS cluster. Fargate launch type provisions your tasks with the required number of CPU cores and gigabytes of memory defined in the task definition.\n Traffic Distribution Betelgeuse’s domain name is orion.com.mx. This domain includes the following subdomains:\n dev.orion.com.mx lab.orion.com.mx test.orion.com.mx api-dev.orion.com.mx api-lab.orion.com.mx api-test.orion.com.mx  Amazon Route 53 is responsible for routing traffic to orion.com.mx and its corresponding subdomains by means of a public hosted zone. A hosted zone contains records that define how the internet traffic is routed for Betelgeuse DNS queries. Amazon Route 53 handles DNS queries and routes traffic to a specific destination of the Betelgeuse application.\nThe Amazon Route 53 service main responsibilities include:\n Mapping domain names to CloudFront distributions, Application Load Balancers and S3 Buckets. Routing traffic for a domain and its subdomains using records. Determining how to respond to DNS queries using a routing policy. Determining the format of the value that returns in response to a DNS query.  The following sections describe in detail how Amazon Route 53 maps domain names to CloudFront distributions and Application Load Balancers.\n Routing Traffic to CloudFront Distributions To describe how Amazon Route 53 distributes traffic to a CloudFront distribution, the following steps use the dev.orion.com.mx subdomain as an example.\n The frontend sends a dev.orion.com.mx DNS query. The Amazon Route 53 service routes traffic for this subdomain to the alternate domain name CNAME associated with the CloudFront distribution.  Note:\nThe DNS query name must exactly match the CloudFront alternate domain name. For example, if the alias record name is dev.orion.com.mx the alternate domain of the CloudFront distribution must be named likewise.  The CloudFront distribution points to the S3 bucket endpoint and serves the client and static content stored in the bucket.  Note:\nThe CloudFront distribution uses an SSL certificate to secure all communications among the AWS resources of the internal network. An SSL certificate is created per domain name using the Amazon ACM service.   Routing Traffic to Application Load Balancers Betelgeuse microservices are placed behind an HTTP/HTTPS load balancer secured by an SSL certificate. The load balancer routes traffic to the microservices based upon inbound rules defined on specific ports. Furthermore, the application load balancers have a security group, which acts as a virtual firewall to control inbound and outbound traffic.\nTo describe how Amazon Route 53 distributes traffic to an application load balancer, the following steps use the api-lab.orion.com.mx subdomain as an example.\n The frontend sends a api-lab.orion.com.mx/bellatrix DNS query. Amazon Route 53 routes all incoming traffic from this DNS name to the application load balancer (ALB) of the laboratory environment. The ALB redirects traffic to a microservice target group based on the route name (/bellatrix/, /rigel/, or /saiph/).  Important:\nThe ALB is configured to listen to ports: 80, 443, 8080, 8443, 8810, and 8811. ALB listeners check for connection requests that use the HTTP or HTTPs protocols on these ports. Each listener includes rules to route incoming traffic to a specific target group.  The target group contains a target of type IP. In this case, the target is the private IP address assigned to the task of the bellatrix microservice. This IP address is dynamic and changes every time the task or the service is restored.  Note:\nAll incoming traffic from ports that use the HTTP protocol is redirected to ports using the HTTPs protocol. This configuration ensures that all incoming requests have an SSL certificate to secure the connection.  For security purposes, each deployment environment has its own Virtual Private Network (VPC) that complies with the network architecture of Orion. A VPC contains three availability zones. Each availability zone includes a private and an isolated subnet for a total of six subnets. Private subnets host container images of the microservices. On the other hand, isolated subnets host databases. Besides, the ALB has an associated SSL certificate to secure communications and encrypt sent data. The Betelgeuse application has an additional security layer, the user must connect to Orion’s VPN to access the microservices resources.\nNote:\nOnly the development environment has both the microservice ECS tasks and databases configured on private subnets. Isolated subnets are not used.   Asynchronous Communication Communication among the microservices that integrate the Betelgeuse application is based on events using both Amazon Simple Notification Service (SNS) topics and Amazon Simple Queue (SQS) service. This strategy ensures a decoupled and asynchronous interaction among the different microservices that follows the principle of request-response messaging for publish-subscribe channels. In this scheme, an event bus collects all the events produced by a microservice. Then, it distributes them among the subscribed queues of other microservices, as Figure 4 shows.\nAs an example, the following steps describe in detail the communication scheme of the bellatrix microservice with the rigel and saiph microservices.\n The bellatrix microservice produces a new event. The Amazon SNS service, that listens to events from the list of topics of all the microservices, performs two actions:  It fans this event out to the list of registered topics of the rigel and saiph microservices. It processes all event logs parallelly and stores them in an S3 bucket for further reference and inspection. To accomplish that,  A Lambda function pushes the events to Amazon Kinesis Data Firehose. The Kinesis Data Firehose service captures and delivers the streaming data (events) into an S3 bucket.\nIf required, a second Lambda function can perform an identity transformation of the events in the stream.     The Amazon SQS service subscribed to the topic collects the new event. The Amazon SQS service appends the event to the queue of the rigel and saiph microservices, respectively. The rigel and saiph microservices process the event. Parallelly, the event is collected by a Death Letter Queue (DLQ) to ensure that the event is processed. A DLQ can handle duplicated events. If required, all DLQ logs can be processed by a Lambda function and stored in an S3 bucket.  Figure 4. Asynchronous and Decoupled Communication among Betelgeuse Microservices.\n Deployment Betelgeuse uses the Octopus tools for deploying the containerized images of the microservices in the cloud. Each deployment environment (development, testing, laboratory, and production) includes five containerized images. Octopus executes a PowerShell script that tells the AWS CLI which processes and resources to implement, as well as which specific version of the images stored in the AWS ECR to use. Figure 5 describes the deployment process of the Betelgeuse application.\nFigure 5. Deployment Process of Betelgeuse Environments using Octopus\nTo understand the deployment process of containerized images, the following steps describe how Octopus deploys the bellatrix image into the development environment.\n A developer makes a pull request to the Orion.Betelgeuse.Bellatrix repository. GitLab CI/CD implements automatically the following stages:  Prebuild  Determines image version using semantic versioning specification v.1.0.0 (SemVer).   Build and Test  Executes unit tests to the .NET project and all its dependencies. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment.   Push  Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab (CI_REGISTRY_IMAGE). Builds, tags, and pushes a dockerized image of the .NET core application to the AWS Elastic Container Registry (ECR). Packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository.   Deploy  Creates a release with the specified version in the Prebuild step and deploys it to the development environment.     Octopus runs the PowerShell script, which contains custom deployment actions to be performed by the AWS CLI.\nOctopus tells AWS ECR to deploy the bellatrix image that matches the specified version number of the created release.  Important:\nOctopus enables custom powershell scripts to have access to the AWS CLI. This requires a previous authentication step to provide AWS credentials to Octopus.   Betelgeuse Security The Betelgeuse application complies with the following security checkpoints:\n General Security Checkpoints  Users must connect to the Orion VPN to access the application. Users must have Office 365 credentials or have a username and password provided by the IT department to enter the application.\nThe IT department can create, organize, and update users directly on the AWS console of Cognito based on a role and permission matrix. Users have restricted permissions to particular Betelgeuse functionalities based on their role and the permission matrix.  Note:\nThe permission matrix applies to each endpoint of every microservice (bellatrix, rigel, and saiph) and to particular functionalities (windows) of the application’s frontend.   Architectural-Level Security Checkpoints  S3 buckets that store the client application have configured access policies. Cloudfront distributions that serve the client application have a SSL certificate. Application Load Balancers that distribute frontend DNS queries to the microservices have a SSL certificate. Each deployment environment has its own VPC including three private and three isolated subnets. Private subnets host the microservices whereas isolated subnets host the databases. Developers can connect to the isolated Amazon Aurora RDS databases from their local machine using an Amazon EC2 instance as a bastion host. This approach enables them to securely administer and give maintenance to the databases. Infrastructure as code of Betelgeuse uses Terraform templates. AWS Systems Manager Parameter Store manages secrets such as passwords, API keys, and other sensitive information (in a string format) that is consumed by the Terraform templates. All requests from the frontend to the backend using the REST API interface require a bearer token. All incoming traffic to the microservices is redirected by the Application Load Balancers to port 443 that uses the HTTPs protocol.   Betelgeuse Monitoring The AWS CloudWatch service monitors the AWS infrastructure of Betelgeuse. This service enables you to select metrics to audit AWS resources such as EC2 instances, logs, SNS messages, SQS queues, CloudFront distributions, Application Load Balancers, and so on. Additionally, Grafana is integrated in the technological stack of Betelgeuse as an open source analytics and monitoring solution to check the AWS infrastructure.\n"});index.add({'id':2,'href':'/docs/howto/','title':"How-To Guide",'content':"Deploying using Octopus in Betelgeuse Last updated: 06/11/2020\n Introduction This guide explains you how to manually deploy to the AWS Cloud any of the following Betelgeuse projects using Octopus deploy:\n Orion.Client Orion.Bellatrix Orion.Rigel Orion.Saiph  Note:\nThese projects are automatically deployed to the development and testing environments. Both environments share the same projects and versions. For the laboratory environment, some of these packages are deployed manually.  The deployment process of Betelgeuse projects is detailed in the Architecture Guide. Briefly, the process consists of four stages that run automatically after a developer makes a pull request in GitLab:\n Prebuild. Determines image version. Build and Test. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment. Push. Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab and to the AWS Elastic Container Registry (ECR). It also packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository. Deploy. Creates a release with the specified version in the Prebuild step and deploys it to a deployment environment.  This guide describes how you can run the Deployment stage manually.\n Log In to Octopus  Go to Octopus Deploy site. Log in to Octopus.  If you are a admin user:  In the Log in page, click on the Google button to sign in using admin SSO. Identify yourself using your admin credentials.   If you are a dev user:  Identify yourself using your dev credentials.      Once you login, you are redirected to the Octopus dashboard that displays all the Betelgeuse projects and their release versions on their respective deployment environments.\n Deploy Betelgeuse Projects Manually This section describes how you can deploy Betelgeuse projects manually using Octopus GUI. As an example, the following steps illustrate the process for deploying the bellatrix microservice manually.\nSelect a Project  In the Octopus dashboard screen, select the Orion.Bellatrix project under the Default Project Group column. The application shows an overview of the project and its versions deployed in each of the deployment environments.  Create a Release  Click on the CREATE RELEASE button, located on the top left corner of the screen under the project’s name.\nOctopus loads the latest version release included in the Octopus Built-in registry pushed from GitLab. It also includes all the packages integrated in this release. Select the version and packages for the release you want to deploy. Click on the SAVE button located at the top right corner of the section. On the left panel, click on Overview.\nOctopus displays the latest release that you created in step 1.  Deploy the Release  Click on the DEPLOY… button under the environment where you want to deploy the release.\nOctopus redirects you to a new screen that shows you step-by-step the progress of the task. Click on the TASK LOG tab to check the deployment logs. Filter the deployment logs with the following parameters:  Expand = All Log level = Verbose Log fail = All Important:\nDeployment logs can help you to monitor and debug the deployment process.     In the top menu, go to Dashboard to verify that the deployment process is ready for the deployment environment you selected.  "});index.add({'id':3,'href':'/docs/','title':"Docs",'content':""});index.add({'id':4,'href':'/docs/season2020/','title':"Proposal Season of Docs 2020",'content':"Welcome to the InterMine Documentation Site InterMine is an open source data warehouse build specifically for the integration and analysis of complex biological data. In this site you can find documentation and tutorials to get you up to speed with our brand new platform BlueGenes.\n Quick Start Learn in 5 min BlueGenes  How-To Guides Know BlueGenes Features  Tutorials Learn a task in BlueGenes    Sample Video Tutorial   Want to Learn More?\nVisit our YouTube channel and have access to all our tutorials.   InterMine User Training Docs | Proposal Overview Provide a brief overview of the project. What problem are you going to solve and how are you going to solve it?\n\u0026ldquo;InterMine is an open source biological data warehouse, designed to help biologists analyse and query data\u0026rdquo;. Currently, the InterMine platform is undergoing a migration process to a new one named BlueGenes. This new modern interface requires the creation of documentation to encourage new and existing end-users to use it. Based on an audit of the InterMine docs and existing resources, there are several opportunity areas for the new documentation of BlueGenes in terms of accessibility, organization, consistency, and updating.\nThe aim of this proposal is to fulfill the documentation requirements of BlueGenes, focusing on a non-technical and semi-technical audience, by following a DocOps strategy. DocOps is a methodology that treats documentation as code fostering a collaborative ecosystem to produce high-quality, centralized, and updated documentation.\nThe BlueGenes documentation will be delivered in a static site powered by Hugo using GitHub Pages as hosting provider. A central repository will store all documentation related to the site such as how-to guides, tutorials, and media resources. The resources will combine text, screenshots, and videos creating a seamless learning experience for the end-users. Click on this link, to consult an example of a documentation site and a video tutorial for BlueGenes.\nThe proposal includes the following sections:\n Statement of the problem. Includes an audit of the current InterMine documentation. Implementation plan. Describes the documentation strategy for InterMine. Timeline and Milestones. Details the step-by-step process for the implementation plan. Deliverables. Lists the documents that can be created. Supporting information. Includes background of the Technical Writer and references that support the proposal.   Statement of the Problem State clearly what the problem is. Please use the project requirements as a guide, but do not copy and paste.\nThis section describes the current state of the InterMine documentation. The following table lists the opportunity areas after auditing the different sources of documentation.\n   Opportunity Area Description     Up-to-date docs Since the creation of the documentation repository in 2014, scattered efforts for adding and/or updating content have been recorded. Particularly, the content for the tutorials section, which was created in 2016, has not been updated since then. The brand new BlueGenes platform requires updated documentation to increase its usage.   Well-ordered docs End-users find it difficult to access relevant information of the InterMine\u0026rsquo;s main features because the documentation is hosted in two different sites. Whereas the InterMine Documentation site consists of tutorials and exercises, the InterMine HomePage allocates tutorials in a video format along with other useful resources in Python. Documentation must exist in a single site having its own repository.   Consistent content The content of tutorials and exercises is composed of different resources such as text, screenshots, and videos. However, not all this content complies with the same template. InterMine documentation site can offer end-users a seamless learning experience. This can be achieved by making the learning resources consistent.   Collaborative docs Four people contribute to the InterMine documentation Rachel Lyne, Joshua Heimbach, Calvin Job Puram, and Yo Yehudi (maintainer). However, there are no clear and specific guidelines of how the community of InterMine users can contribute to documentation (a how-to guide to contribute). Besides, the tone, voice, and style of InterMine documentation is not defined. A style guide can foster community interaction with the documentation site to upload, renew, and fix content in a systematic way.   Platform compatibility The InterMine HomePage uses Hugo as a static generator whereas the InterMine Documentation site uses Sphinx. Both sites must use the same infrastructure to ease the creation, collaboration, and maintenance of the InterMine documentation site.   Resources compatibility Media resources are hosted on different locations such as YouTube and TechSmith screencast. Besides, the duration and format of the videos is not consistent. Some videos use tag description while others use a voice-over description format. By defining the infrastructure, specific guidelines, and a style guide, all resources can be consistent and user-friendly.     Implementation Plan Try to be concise, but do explain your plan thoroughly.\nThe implementation plan for this project consists of four stages:\n Agreements Stage Working Stage  Setting up Infrastructure Documenting the main features of BlueGenes Documenting the main Biology-related topics Documenting Tutorials that cover the main tasks that can be done with BlueGenes.   Closing Stage  Documenting a Product Overview of BlueGenes Documenting InterMine\u0026rsquo;s Contributors Guide (optional)   Final Stage  Creating Project\u0026rsquo;s Report Completing Mentor\u0026rsquo;s Assessments    Agreements Stage\nThis stage refines the main goals, tasks, and deliverables of this proposal taking into account the documentation risks. I will start reviewing the opportunity areas for the InterMine documentation with the team. My interest is to foster a very close collaboration with the team and members of the community and identify all sources of information to document. Together, we will settle down the foundations of the project\u0026rsquo;s infrastructure to build and create the docs.\nWorking Stage\nThis stage focuses on setting up the infrastructure of the documentation site and documenting the main features of BlueGenes. I will explore and use the BlueGenes platform by myself with the fresh eyes of an end-user. This will give me the freedom to start documenting the main features that are already on the production stage. Fig. 1 shows the documentation lifecycle that I will follow.\nFig. 1 Documentation Lifecycle for InterMine.\nClosing Stage\nThis stage focuses on the creation of a product overview of BlueGenes. Having created all the content of the site, I will produce a getting started section where end-users can get up to speed with the tools that BlueGenes offers. This can be a five-minute tutorial section. As a final deliverable, I will give a demo session to the InterMine\u0026rsquo;s community showing the documentation generated in the static site. If time allows, this stage includes the creation of a How-to guide for contributors.\nFinal Stage\nThis stage includes all paperwork related to the project´s closure.\n Timeline: Milestones Cover September to December 2020 milestones.\nThis section describes the milestones for the project based on the implementation plan.\nCommunity bonding\t(August 17, 2020 - September 13, 2020)\nDuring this stage of the project, I suggest the following activities:\n Get to know InterMine\u0026rsquo;s mentors to foster team collaboration. Refine goals, tasks, and deliverables of this proposal. Run an investigation period to define:  InterMine\u0026rsquo;s documentation infrastructure Style and tone for the documentation Media style and format    Doc development (September 14, 2020 - November 30, 2020)\nThe following table describes the milestones and timeline for this proposal.\n  Deliverables and Tasks      1. Setting up documentation infrastructure [1 week]      Static site infrastructure      Hosting infrastructure for static content such as videos      CI/CD tools (if required)    2. Documenting BlueGenes main features [4 weeks]      How-to guide for MyMind Account      How-to guides for searching tools        Keyword Search        Template Search        Query Builder        Region Search        List Search      How-to guides for viewing data tools        Report Pages        List Analysis Pages        Result Tables        Region Search Results        List Analysis Pages    3. Documenting How-to guides for Biology-related content [4 weeks]      Gene Ontology      Expression Data      Pathways      Regulatory Data      Orthologues      Interactions    4. Documenting Tutorials [3 weeks]    5. Documenting BlueGene’s Product Overview [1 week]    6. Documenting a How-to guide to contribute to InterMine’s documentation (optional) [1 week]      Define a Style Guide      Create README for the main repository      Integrate VALE as a tool to validate documentation’s style and tone    7. Work on Final Report [1 week]    8. Complete Mentor’s evaluations [1 week]     Deliverables Indicate which deliverables are required and which are optional. Include presentation of your project on the InterMine community call as a deliverable.\nThis section lists some of the deliverables included for this proposal.\n A static site powered by Hugo and hosted in GitHub Pages. How-to guides for the main features of BlueGenes. How-to guides for the main Biology-related topics. Tutorials that cover the main tasks that can be done with BlueGenes. A Product Overview of BlueGenes. A how-to guide to contribute to InterMine\u0026rsquo;s documentation (optional). A Style Guide for InterMine\u0026rsquo;s documentation (optional). VALE as a tool to check the fundamentals of the style guide (optional).   Documentation Risks Assumptions\nThe technical writer assumes the following:\n At least one stakeholder from InterMine must review and approve all deliverable documents. Documentation is delivered in a static site generator.  Dependencies and Risks\nThe InterMine documentation has the following dependencies and risks:\n Documentation delivery estimates are subject to change based on the scope of the project. Late changes of the scope and requirements can prevent timely delivery of high-quality documentation. Late feedback for documentation in the reviewing process can affect the high-quality and timely delivery of the documentation.  "});})();